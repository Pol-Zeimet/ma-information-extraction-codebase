import os
from typing import Dict, Any, Optional
from src.experiments.config import Config
from src.models.model_architectures.bert_architecture import Bert
import numpy as np
from dataclasses import dataclass, field
from transformers import Trainer, \
    DataCollatorForTokenClassification, \
    AutoModelForTokenClassification, \
    PreTrainedTokenizerBase
from src.util.metrics.baseline_metrics import accuracy_score, recall_score, precision_score, f1_score
import mlflow


class BertConfig(Config):
    def __init__(self,
                 model_dir: str,
                 model_id: str,
                 model_type: str,
                 label_list: [str],
                 num_classes: int,
                 train_f: str,
                 test_f: str,
                 validate_f: str,
                 logging: bool = True):
        super().__init__(logging=logging,
                         num_classes=num_classes)
        self.model_dir = model_dir
        self.model_id = model_id
        self.model_type = model_type
        self.label_list = label_list
        self.train_f = train_f
        self.test_f = test_f
        self.validate_f = validate_f


@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """

    model_name_or_path: str = field(
        default='bert-base-uncased',
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None,
        metadata={"help": "Where do you want to store the pretrained models downloaded from huggingface.co"},
    )


@dataclass
class DataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """
    task_name: Optional[str] = field(default="ner", metadata={"help": "The name of the task (ner, pos...)."})
    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    dataset_config_name: Optional[str] = field(
        default=None, metadata={"help": "The configuration name of the dataset to use (via the datasets library)."}
    )
    train_file: Optional[str] = field(
        default='/content/drive/MyDrive/Master_Arbeit/colab/data/data/train.json',
        metadata={"help": "The input training data file (a csv or JSON file)."}
    )
    validation_file: Optional[str] = field(
        default='/content/drive/MyDrive/Master_Arbeit/colab/data/data/validate.json',
        metadata={"help": "An optional input evaluation data file to evaluate on (a csv or JSON file)."},
    )

    test_file: Optional[str] = field(
        default=None,
        metadata={"help": "An optional input test data file to predict on (a csv or JSON file)."},
    )
    overwrite_cache: bool = field(
        default=False, metadata={"help": "Overwrite the cached training and evaluation sets"}
    )
    preprocessing_num_workers: Optional[int] = field(
        default=1,
        metadata={"help": "The number of processes to use for the preprocessing."},
    )
    pad_to_max_length: bool = field(
        default=False,
        metadata={
            "help": "Whether to pad all samples to model maximum sentence length. "
                    "If False, will pad the samples dynamically when batching to the maximum length in the batch. More "
                    "efficient on GPU but very bad for TPU."
        },
    )
    label_all_tokens: bool = field(
        default=False,
        metadata={
            "help": "Whether to put the label for one word on all tokens of generated by that word or just on the "
                    "one (in which case the other tokens will have a padding index)."
        },
    )

    def __post_init__(self):
        if self.dataset_name is None and self.train_file is None and self.validation_file is None:
            raise ValueError("Need either a dataset name or a training/validation file.")
        else:
            if self.train_file is not None:
                extension = self.train_file.split(".")[-1]
                assert extension in ["csv", "json"], "`train_file` should be a csv or a json file."
            if self.validation_file is not None:
                extension = self.validation_file.split(".")[-1]
                assert extension in ["csv", "json"], "`validation_file` should be a csv or a json file."
        self.task_name = self.task_name.lower()


class BertModel:
    def __init__(self, config: BertConfig, training_args, data_args, model_args):
        self.config = config
        self.logging = config.logging

        self.label_list = config.label_list
        self.model_id = config.model_id
        self.model_type = config.model_type

        self.training_args = training_args
        self.data_args = data_args
        self.model_args = model_args

        self.model: AutoModelForTokenClassification = None
        self.tokenizer: PreTrainedTokenizerBase = None
        self.trainer: Trainer = None
        self.create_model_architecture()

    def get_details(self) -> Dict[str, Any]:
        return {"model": "Bert",
                'number of classes': self.config.num_classes,
                }

    def create_model_architecture(self) -> None:
        self.tokenizer, self.model = Bert.create(self.data_args, self.model_args, self.config.num_classes)

    def train(self, data):
        data_collator = DataCollatorForTokenClassification(self.tokenizer)
        self.trainer = Trainer(
            model=self.model,
            args=self.training_args,
            train_dataset=data['train'],
            eval_dataset=data['validation'],
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
        )
        self.trainer.train(self.model_args.model_name_or_path
                           if os.path.isdir(self.model_args.model_name_or_path)
                           else None)

    def evaluate(self):
        return self.trainer.evaluate()

    def predict(self, datasets):
        test_dataset = datasets["test"]
        predictions, labels, metrics = self.trainer.predict(test_dataset)
        predictions = np.argmax(predictions, axis=2)
        return predictions

    def save_weights(self, path: str) -> None:
        self.trainer.save_model(path)
        print("Model saved in " + path)

    def compute_metrics(self, p):
        predictions, labels = p
        predictions = np.argmax(predictions, axis=2)

        # Remove ignored index (special tokens)
        true_predictions = [
            [self.label_list[p] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]
        true_labels = [
            [self.label_list[l] for (p, l) in zip(prediction, label) if l != -100]
            for prediction, label in zip(predictions, labels)
        ]

        acc = accuracy_score(true_labels, true_predictions)
        precision = precision_score(true_labels, true_predictions)
        recall = recall_score(true_labels, true_predictions)
        f1 = f1_score(true_labels, true_predictions)

        mlflow.log_metric("accuracy", acc)
        mlflow.log_metric("f1", f1)
        mlflow.log_metric("recall", recall)
        mlflow.log_metric("precision", precision)

        return {
            "accuracy_score": acc,
            "precision": precision,
            "recall": recall,
            "f1": f1
        }

    def tokenize_and_align_labels(self, examples, **kwargs):
        text_column_name = kwargs['text_column_name']
        label_column_name = kwargs['label_column_name']
        label_to_id = kwargs['label_to_id']

        padding = "max_length" if self.data_args.pad_to_max_length else False
        tokenized_inputs = self.tokenizer(
            examples[text_column_name],
            padding=padding,
            truncation=True,
            # We use this argument because the texts in our dataset are lists of words (with a label for each word).
            is_split_into_words=True,
            return_offsets_mapping=True,
        )

        offset_mappings = tokenized_inputs.pop("offset_mapping")
        labels = []
        for label, offset_mapping in zip(examples[label_column_name], offset_mappings):
            label_index = 0
            current_label = -100
            label_ids = []
            for offset in offset_mapping:
                # We set the label for the first token of each word. Special characters will have an offset of (0, 0)
                # so the test ignores them.
                if offset[0] == 0 and offset[1] != 0:
                    current_label = label_to_id[label[label_index]]
                    label_index += 1
                    label_ids.append(current_label)
                # For special tokens, we set the label to -100 so it's automatically ignored in the loss function.
                elif offset[0] == 0 and offset[1] == 0:
                    label_ids.append(-100)
                # For the other tokens in a word, we set the label to either the current label or -100, depending on
                # the label_all_tokens flag.
                else:
                    label_ids.append(current_label if self.data_args.label_all_tokens else -100)
            labels.append(label_ids)
        tokenized_inputs["labels"] = labels
        return tokenized_inputs
